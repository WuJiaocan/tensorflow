{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "SRC_TRAIN_DATA = \"./data/train.en\"\n",
    "TRG_TRAIN_DATA = \"./data/train.zh\"\n",
    "CHECKPOINT_PATH = \"./seq2seq_ckpt\"\n",
    "\n",
    "HIDDEN_SIZE = 1024\n",
    "NUM_LAYERS = 2\n",
    "SRC_VOCAB_SIZE = 10000\n",
    "TRG_VOCAB_SIZE = 4000\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCH = 5\n",
    "KEEP_PROB = 0.8\n",
    "MAX_GRAD_NORM = 5\n",
    "SHARE_EMB_AND_SOFTMAX = True\n",
    "\n",
    "MAX_LEN = 50\n",
    "SOS_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeDataset(file_path):\n",
    "    dataset = tf.data.TextLineDataset(file_path)\n",
    "    dataset = dataset.map(lambda string: tf.string_split([string]).values)\n",
    "    dataset = dataset.map(lambda string: tf.string_to_number(string, tf.int32))\n",
    "    dataset = dataset.map(lambda x:(x, tf.size(x)))\n",
    "    return dataset\n",
    "\n",
    "def MakeSrcTrgDataset(src_path, trg_path, batch_size):\n",
    "    src_data = MakeDataset(src_path)\n",
    "    trg_data = MakeDataset(trg_path)\n",
    "    dataset = tf.data.Dataset.zip(src_data, trg_data)\n",
    "    \n",
    "    def FilterLength(src_tuple, trg_tuple):\n",
    "        ((src_input, src_len), (trg_label, trg_len)) = (src_tuple, trg_tuple)\n",
    "        src_len_ok = tf.logical_and(\n",
    "            tf.greater(src_len, 1), tf.less_equal(src_len, MAX_LEN))\n",
    "        trg_len_ok = tf.logical_and(\n",
    "            tf.greater(trg_len, 1), tf.less_equal(trg_len, MAX_LEN))\n",
    "        return tf.logical_and(src_len_ok, trg_len_ok)\n",
    "    dataset = dataset.filter(FilterLength)\n",
    "    \n",
    "    def MakeTrgInput(src_tuple, trg_tuple):\n",
    "        ((src_input, src_len), (trg_label, trg_len)) = (src_tuple, trg_tuple)\n",
    "        trg_input = tf.concat([[SOS_ID], trg_label[:-1]], axis=0)\n",
    "        return ((src_input, src_len), (trg_input, trg_label, trg_len))\n",
    "    dataset = dataset.map(MakeTrgInput)\n",
    "    dataset = dataset.shuffle(10000)\n",
    "    padded_shapes = (\n",
    "        (tf.TensorShape([None]),\n",
    "         tf.TensorShape([])),\n",
    "        (tf.TensorShape([None]),\n",
    "         tf.TensorShape([None]),\n",
    "         tf.TensorShape([])))\n",
    "    batched_dataset = dataset.padded_batch(batch_size, padded_shapes)\n",
    "    \n",
    "    return batched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTModel(object):\n",
    "    def __init__(self):\n",
    "        self.enc_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE) for _ in range(NUM_LAYERS)])\n",
    "        self.dec_cell = tf.nn.rnn_cell.MultiRNNCelll(\n",
    "            [tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE) for _ in range(NUM_LAYERS)])\n",
    "        \n",
    "        self.src_embedding = tf.get_variable(\n",
    "            \"src_emb\", [SRC_VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        self.trg_embedding = tf.get_variable(\n",
    "            \"trg_emb\", [TRG_VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        \n",
    "        if SHARE_EMB_AND_SOFTMAX:\n",
    "            self.softmax_weight = tf.transpose(self.trg_embedding)\n",
    "        else:\n",
    "            self.softmax_weight = tf.get_variable(\n",
    "                \"weight\", [HIDDEN_SIZE, TRG_VOCAB_SIZE])\n",
    "        self.softmax_bias = tf.get_variable(\n",
    "            \"softmax_bias\", [TRG_VOCAB_SIZE])\n",
    "    def forword(self, src_input, src_size, trg_input, trg_label, trg_size):\n",
    "        batch_size = tf.shape(src_input)[0]\n",
    "        \n",
    "        src_emb = tf.nn.embedding_lookup(self.src_embedding, src_input)\n",
    "        trg_emb = tf.nn.embedding_lookup(self.trg_embedding, trg_input)\n",
    "        \n",
    "        src_emb = tf.nn.dropout(src_emb, KEEP_PROB)\n",
    "        trg_emb = tf.nn.dropout(trg_emb, KEEP_PROB)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTModel(object):\n",
    "    def forword(self, src_input, src_size, trg_input, trg_label, trg_size):\n",
    "     \n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            enc_outputs = enc_state = tf.nn.dynamic_rnn(\n",
    "                self.enc_cell, src_emb, src_size, dtype=tf.float32)\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            dec_outputs, _ = tf.nn.dynamic_rnn(\n",
    "                self.dec_cellm trg_emb, trg_size, initial_state=enc_state)\n",
    "        \n",
    "        output = tf.reshape(dec_outputs, [-1, HIDDEN_SIZE])     # [ batch_size * num_steps, HIDDEN_SIZE ]\n",
    "        logits = tf.matmul(output, self.softmax_weight) + self.softmax_bias\n",
    "        # tf.reshape(trg_labels,[-1]) 格式是一行，batch_size * num_steps列，即这一个batch里所有的元素\n",
    "        # logits的格式是 batch_size * num_steps行，TRG_VOCAB_SIZE列，即这个batch里所有元素预测的结果映射成\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=tf.reshape(trg_labels,[-1]), logits=logits) # trg_labels是trg_input的下一位，就是LSTM预测的outputs\n",
    "        \n",
    "        \n",
    "        \n",
    "class NMTModel(object):\n",
    "        def forward(self, src_input, src_size, trg_input, trg_label, trg_size):\n",
    "            output = tf.reshape(dec_outputs, [-1, HIDDEN_SIZE])\n",
    "            logits = tf.matmul(output, self.softmax_weight) + self.softmax_bias\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                lables=tf.reshape(trg_labels, [-1]), logits=logits)\n",
    "            \n",
    "            label_weights = tf.sequence_mask(\n",
    "                trg_size, maxlen=tf.shape(trg_label)[1], dtype=tf.float32)  # padding部分为False.转成float,0.0\n",
    "            label_weights = tf.reshape(label_weights, [-1])                 # 转成一维数据\n",
    "            cost = tf.reduce_sum(loss*label_weights)\n",
    "            cost_per_token = cost / tf.reduce_sum(label_weights)\n",
    "            \n",
    "            trainable_variables = tf.trainable_variables()\n",
    "            \n",
    "            \n",
    "            grads, _ = tf.clip_by_global_norm(grads, MAX_GRAD_NORM)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n",
    "            train_op = optimizer.apply_gradients(\n",
    "                zip(grads, trainable_variables))\n",
    "            return cost_per_token, train_op\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# a = tf.constant([1.,2.,3.,0.,9.,])\n",
    "# b = tf.constant([[1,2,3],[3,2,1],[4,5,6],[6,5,4]])\n",
    "# with tf.Session() as sess:\n",
    "#     print(sess.run(tf.argmax(a, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.enc_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "    [tf.nn.rnn_cell.BasicLSTMCell(HIDDENZ_SIZE) for _ in NUM_LAYERS])\n",
    "\n",
    "self.enc_cell_fw = tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)\n",
    "self.enc_cell_bw = tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)\n",
    "\n",
    "############################################################################\n",
    "\n",
    "with tf.variable_scope(\"encoder\"):\n",
    "    enc_outputs, enc_state = tf.nn.dynamic_rnn(\n",
    "        self.enc_cell, src_emb, src_size, dtype=tf.float32)\n",
    "    \n",
    "with tf.variable_scope(\"encoder\"):\n",
    "    enc_outputs, enc_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        self.enc_cell_fw, self.enc_cell_bw, src_emb, src_size, dtype=tf.float32)\n",
    "    enc_outputs = tf.concat([enc_outputs[0], enc_outputs[1]], -1)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "with tf.variable_scope(\"decoder\"):\n",
    "    dec_outputs, _ = tf.nn.dynamic_rnn(\n",
    "        self.dec_cell, trg_emb, trg_size, initial_state = enc_state)\n",
    "    \n",
    "with tf.variable_scope(\"decoder\"):\n",
    "    attention_mechanism = tf.contrib.seq2seq.BahdanauMonotonicAttention(\n",
    "        HIDDEN_SIZE, enc_outputs, memory_sequence_length=src_size)\n",
    "    atten_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        self.dec_cell, attention_mechanism, attention_layer_size = HIDDEN_SIZE)\n",
    "    dec_outputs, _ = tf.nn.dynamic_rnn(\n",
    "        atten_cell, trg_emb, trg_size, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder\"):\n",
    "    dec_outputs, _ = tf.nn.dynamic_rnn(\n",
    "        self.dec_cell, trg_emb, trg_size, initial_state = enc_state)\n",
    "##改成##\n",
    "with tf.variable_scope(\"encoder\"):\n",
    "    enc_outputs, enc_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        self.enc_cell_fw, self.enc_cell_bw, src_emb, src_size, dtype=tf.float32)\n",
    "    enc_outputs = tf.concat([enc_outputs[0], enc_outputs[1]], -1)\n",
    "\n",
    "with tf.variable_scope(\"decoder\"):\n",
    "    attention_mechanism = tf.contrib.seq2seq.BahdanauMonotonicAttentiond(\n",
    "        HIDDEN_SIZE, enc_outputs, memory_sequence_length=src_size)\n",
    "    attention_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        self.dec_cell, attention_mechanism, attention_layer_size=HIDDEN_SIZE)\n",
    "    dec_outputs, _ = tf.nn.dynamic_rnn(\n",
    "        attention_cell, trg_emb, trg_size, dtype=tf.float32)\n",
    "###################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((array([  90,   13, 1814,    0,    4,   11,   86, 6045,    0,    4,    2],\n",
      "      dtype=int32), 11), (array([  1,  30, 787, 148, 931, 630, 104,  10, 235,   7, 144, 300, 148,\n",
      "       680, 502,   4,   5,   7,  30, 787, 148, 931, 630,   6], dtype=int32), array([ 30, 787, 148, 931, 630, 104,  10, 235,   7, 144, 300, 148, 680,\n",
      "       502,   4,   5,   7,  30, 787, 148, 931, 630,   6,   2], dtype=int32), 24))\n"
     ]
    }
   ],
   "source": [
    "src_data = MakeDataset(SRC_TRAIN_DATA)\n",
    "trg_data = MakeDataset(TRG_TRAIN_DATA)\n",
    "dataset = tf.data.Dataset.zip((src_data, trg_data))\n",
    "dataset = dataset.filter(FilterLength)\n",
    "dataset = dataset.map(MakeTrgInput)\n",
    "# dataset = dataset.shuffle(10000)\n",
    "# padded_shapes = (\n",
    "#     (tf.TensorShape([None]),\n",
    "#      tf.TensorShape([])),\n",
    "#     (tf.TensorShape([None]),\n",
    "#      tf.TensorShape([None]),\n",
    "#      tf.TensorShape([])))\n",
    "# batched_dataset = dataset.padded_batch(100, padded_shapes)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "one_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(1):\n",
    "        print(sess.run(one_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument 0 has invalid type <class 'int'>, must be a string or Tensor. (Can not convert a int into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    281\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 282\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    283\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3338\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3339\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3427\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" % (type(obj).__name__,\n\u001b[0;32m-> 3428\u001b[0;31m                                                            types_str))\n\u001b[0m\u001b[1;32m   3429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a int into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-1f4d9e3204ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trg_label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1085\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \"\"\"\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    284\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n\u001b[1;32m    285\u001b[0m                         \u001b[0;34m'must be a string or Tensor. (%s)'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                         (fetch, type(fetch), str(e)))\n\u001b[0m\u001b[1;32m    287\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument 0 has invalid type <class 'int'>, must be a string or Tensor. (Can not convert a int into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "data = MakeSrcTrgDataset(SRC_TRAIN_DATA, TRG_TRAIN_DATA, 10)\n",
    "iterator = data.make_one_shot_iterator()\n",
    "iterator = iterator.get_next()\n",
    "# iterator = data.make_initializable_iterator()\n",
    "# (src, src_size), (trg_input, trg_label, trg_size) = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(1):\n",
    "        print(sess.run(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((array([[  90,   13, 1814, ...,    0,    0,    0],\n",
      "       [  18,   17,   41, ...,    0,    0,    0],\n",
      "       [  55,   85,  123, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 298, 4644,  260, ...,    0,    0,    0],\n",
      "       [2042, 1341,   13, ...,    0,    0,    0],\n",
      "       [ 126,   16,   82, ...,    0,    0,    0]], dtype=int32), array([11, 17, 30, 34, 17, 16,  9,  9, 16, 10, 19, 22, 25, 28, 16, 27, 26,\n",
      "        6, 18, 13, 10,  9,  8, 18, 12,  7, 14, 24, 41, 16,  8, 17, 15, 10,\n",
      "       25, 27,  7, 20,  9, 16, 14, 14, 25, 19, 19, 10, 13, 12, 12, 12, 33,\n",
      "       10, 12, 32, 17, 17, 39, 23, 12, 21, 19,  7, 11, 13, 15, 34, 35, 12,\n",
      "       44, 11, 10, 27, 15, 23, 16, 18, 16,  9, 12, 24, 15, 19, 17, 28, 11,\n",
      "       24, 20, 11,  7, 13, 10, 14, 12, 14,  6, 23, 28, 10, 36,  9],\n",
      "      dtype=int32)), (array([[  1,  30, 787, ...,   0,   0,   0],\n",
      "       [  1,   5,   8, ...,   0,   0,   0],\n",
      "       [  1,   5,   8, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [  1, 144, 300, ...,   0,   0,   0],\n",
      "       [  1,  33,   8, ...,   0,   0,   0],\n",
      "       [  1,  33,   8, ...,   0,   0,   0]], dtype=int32), array([[ 30, 787, 148, ...,   0,   0,   0],\n",
      "       [  5,   8, 133, ...,   0,   0,   0],\n",
      "       [  5,   8,  10, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [144, 300, 104, ...,   0,   0,   0],\n",
      "       [ 33,   8,   3, ...,   0,   0,   0],\n",
      "       [ 33,   8,  24, ...,   0,   0,   0]], dtype=int32), array([24, 21, 27, 35, 18, 19, 12, 12, 17, 16, 27, 21, 39, 33, 24, 40, 41,\n",
      "        8, 16, 14, 14, 13, 10, 20, 16, 10, 31, 29, 45, 17, 11, 22, 19, 13,\n",
      "       30, 40, 11, 29, 12, 21, 23, 16, 31, 25, 28, 15, 16, 18, 18, 20, 42,\n",
      "       15, 16, 38, 30, 20, 47, 28, 14, 27, 18,  9, 11, 15, 20, 40, 47, 18,\n",
      "       48, 17, 12, 32, 18, 27, 19, 18, 24, 14, 15, 29, 30, 14, 18, 42, 20,\n",
      "       25, 29, 16, 11, 21, 11, 23, 13, 15,  6, 25, 36, 21, 43, 11],\n",
      "      dtype=int32)))\n"
     ]
    }
   ],
   "source": [
    "src_data = MakeDataset(SRC_TRAIN_DATA)\n",
    "trg_data = MakeDataset(TRG_TRAIN_DATA)\n",
    "dataset = tf.data.Dataset.zip((src_data, trg_data))\n",
    "dataset = dataset.filter(FilterLength)\n",
    "dataset = dataset.map(MakeTrgInput)\n",
    "# dataset = dataset.shuffle(10000)\n",
    "padded_shapes = (\n",
    "    (tf.TensorShape([None]),\n",
    "     tf.TensorShape([])),\n",
    "    (tf.TensorShape([None]),\n",
    "     tf.TensorShape([None]),\n",
    "     tf.TensorShape([])))\n",
    "batched_dataset = dataset.padded_batch(100, padded_shapes)\n",
    "\n",
    "iterator = batched_dataset.make_one_shot_iterator()\n",
    "\n",
    "one_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(1):\n",
    "        print(sess.run(one_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "a = tf.concat([[1,2,3],[2,3,4]], axis=0)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "max_value = tf.reduce_max([1, 3, 2])\n",
    "with tf.Session() as sess:\n",
    "    max_value = sess.run(max_value)\n",
    "    print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 创建Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.35242986, 0.02526439])]\n",
      "[array([0.76184762, 0.45632948])]\n",
      "[array([0.98879853, 0.79668701])]\n",
      "[array([0.2534393 , 0.04780475])]\n",
      "[array([0.22052244, 0.20931121])]\n",
      "[array([0.12144788, 0.00884343])]\n",
      "[array([0.31898259, 0.1008208 ])]\n",
      "[array([0.31425952, 0.55162422])]\n",
      "[array([0.39618981, 0.51365267])]\n",
      "[array([0.29278261, 0.47394459])]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.uniform(size=(100,2))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(a)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(10):\n",
    "        print(sess.run([one_element]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 处理dict输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': 1.0, 'b': array([0.78272703, 0.9589407 ])}]\n",
      "[{'a': 2.0, 'b': array([0.88678781, 0.3501535 ])}]\n",
      "[{'a': 3.0, 'b': array([0.76154157, 0.24016552])}]\n",
      "[{'a': 4.0, 'b': array([0.64967107, 0.51682127])}]\n",
      "[{'a': 5.0, 'b': array([0.92342321, 0.3633148 ])}]\n"
     ]
    }
   ],
   "source": [
    "b = {\"a\":np.array([1.0,2.0,3.0,4.0,5.0]),\n",
    "     \"b\": np.random.uniform(size=(5,2))}\n",
    "dataset = tf.data.Dataset.from_tensor_slices(b)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(5):\n",
    "        print(sess.run([one_element]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 处理tuple输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array([1.0,2.0,3.0,4.0,5.0]), np.random.uniform(size=(5,2))))\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(5):\n",
    "        print(sess.run([one_element]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 创建iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.uniform(size=(100,2))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(a)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator() # one-shot\n",
    "one_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(10):\n",
    "        print(sess.run([one_element]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# Initialize an iterator over a dataset with 10 elements.\n",
    "sess.run(iterator.initializer, feed_dict={max_value: 10})\n",
    "for i in range(10):\n",
    "    value = sess.run(next_element)\n",
    "    assert i == value\n",
    "    print value\n",
    "\n",
    "# Initialize the same iterator over a dataset with 100 elements.\n",
    "sess.run(iterator.initializer, feed_dict={max_value: 100})\n",
    "for i in range(100):\n",
    "    value = sess.run(next_element)\n",
    "    assert i == value\n",
    "    print value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': 2.0, 'b': array([0.67249248, 0.7504482 ])}]\n",
      "[{'a': 3.0, 'b': array([0.53606116, 0.35014902])}]\n",
      "[{'a': 4.0, 'b': array([0.17724591, 0.27518901])}]\n",
      "[{'a': 5.0, 'b': array([0.59307067, 0.6250142 ])}]\n",
      "[{'a': 6.0, 'b': array([0.31187147, 0.04594253])}]\n"
     ]
    }
   ],
   "source": [
    "b = {\"a\":np.array([1.0,2.0,3.0,4.0,5.0]),\n",
    "     \"b\":np.random.uniform(size=(5,2))}\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(b)\n",
    "dataset = dataset.map(lambda x:{'a':x['a']+1, 'b':x['b']})\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(5):\n",
    "        print(sess.run([one_element]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': array([1., 2.]), 'b': array([[0.41660447, 0.39681735],\n",
      "       [0.35483807, 0.00966841]])}]\n",
      "[{'a': array([3., 4.]), 'b': array([[0.53269592, 0.90459206],\n",
      "       [0.92989925, 0.65310525]])}]\n",
      "[{'a': array([5.]), 'b': array([[0.57499478, 0.60156916]])}]\n"
     ]
    },
    {
     "ename": "OutOfRangeError",
     "evalue": "End of sequence\n\t [[Node: IteratorGetNext_25 = IteratorGetNext[output_shapes=[[?], [?,2]], output_types=[DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_16)]]\n\nCaused by op 'IteratorGetNext_25', defined at:\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-60-7b0c063fa8fc>\", line 6, in <module>\n    one_element = iterator.get_next()\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 410, in get_next\n    name=name)), self._output_types,\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2069, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nOutOfRangeError (see above for traceback): End of sequence\n\t [[Node: IteratorGetNext_25 = IteratorGetNext[output_shapes=[[?], [?,2]], output_types=[DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_16)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence\n\t [[Node: IteratorGetNext_25 = IteratorGetNext[output_shapes=[[?], [?,2]], output_types=[DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_16)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-7b0c063fa8fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mone_element\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence\n\t [[Node: IteratorGetNext_25 = IteratorGetNext[output_shapes=[[?], [?,2]], output_types=[DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_16)]]\n\nCaused by op 'IteratorGetNext_25', defined at:\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-60-7b0c063fa8fc>\", line 6, in <module>\n    one_element = iterator.get_next()\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 410, in get_next\n    name=name)), self._output_types,\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2069, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/wujiaocan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nOutOfRangeError (see above for traceback): End of sequence\n\t [[Node: IteratorGetNext_25 = IteratorGetNext[output_shapes=[[?], [?,2]], output_types=[DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_16)]]\n"
     ]
    }
   ],
   "source": [
    "b = {'a':np.array([1.0,2.0,3.0,4.0,5.0]),\n",
    "     'b':np.random.uniform(size=(5,2))}\n",
    "dataset = tf.data.Dataset.from_tensor_slices(b)\n",
    "dataset = dataset.batch(2)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(5):\n",
    "        print(sess.run([one_element]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': 2.0, 'b': array([0.4740369 , 0.42298609])}]\n",
      "[{'a': 3.0, 'b': array([0.06155087, 0.71266742])}]\n",
      "[{'a': 4.0, 'b': array([0.47337352, 0.06822952])}]\n",
      "[{'a': 5.0, 'b': array([0.65602231, 0.14035671])}]\n",
      "[{'a': 1.0, 'b': array([0.69665099, 0.15347363])}]\n"
     ]
    }
   ],
   "source": [
    "b = {\"a\":np.array([1.0,2.0,3.0,4.0,5.0]),\n",
    "     \"b\": np.random.uniform(size=(5,2))}\n",
    "# 创建dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(b)\n",
    "# dataset = dataset.batch(2)\n",
    "dataset = dataset.shuffle(4)\n",
    "# 创建Iterator读取数据\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(5):\n",
    "        print(sess.run([one_element]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': 1.0, 'b': array([0.11900588, 0.84215683])}]\n",
      "[{'a': 4.0, 'b': array([0.72793599, 0.12957333])}]\n",
      "[{'a': 3.0, 'b': array([0.4489395, 0.7241493])}]\n",
      "[{'a': 5.0, 'b': array([0.9213787 , 0.61327884])}]\n",
      "[{'a': 2.0, 'b': array([0.72173393, 0.17270813])}]\n",
      "[{'a': 4.0, 'b': array([0.72793599, 0.12957333])}]\n",
      "[{'a': 5.0, 'b': array([0.9213787 , 0.61327884])}]\n",
      "[{'a': 3.0, 'b': array([0.4489395, 0.7241493])}]\n",
      "[{'a': 2.0, 'b': array([0.72173393, 0.17270813])}]\n",
      "[{'a': 3.0, 'b': array([0.4489395, 0.7241493])}]\n"
     ]
    }
   ],
   "source": [
    "b = {\"a\":np.array([1.0,2.0,3.0,4.0,5.0]),\n",
    "     \"b\": np.random.uniform(size=(5,2))}\n",
    "# 创建dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(b)\n",
    "dataset = dataset.repeat(5)\n",
    "dataset = dataset.shuffle(4)\n",
    "# 创建Iterator读取数据\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(10):\n",
    "        print(sess.run([one_element]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batched_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 0 0]\n",
      " [2 2 0]\n",
      " [3 3 3]]\n",
      "[[4 4 4 4 0 0 0]\n",
      " [5 5 5 5 5 0 0]\n",
      " [6 6 6 6 6 6 0]\n",
      " [7 7 7 7 7 7 7]]\n",
      "[[ 8  8  8  8  8  8  8  8  0  0  0]\n",
      " [ 9  9  9  9  9  9  9  9  9  0  0]\n",
      " [10 10 10 10 10 10 10 10 10 10  0]\n",
      " [11 11 11 11 11 11 11 11 11 11 11]]\n",
      "[[12 12 12 12 12 12 12 12 12 12 12 12  0  0  0]\n",
      " [13 13 13 13 13 13 13 13 13 13 13 13 13  0  0]\n",
      " [14 14 14 14 14 14 14 14 14 14 14 14 14 14  0]\n",
      " [15 15 15 15 15 15 15 15 15 15 15 15 15 15 15]]\n",
      "[[16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16  0  0  0]\n",
      " [17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17 17  0  0]\n",
      " [18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18  0]\n",
      " [19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19 19]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "dataset = tf.data.Dataset.range(100)\n",
    "dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x)) # tf.fill(dims, value, name=None)创建一个维度为dims，值为value的tensor对象\n",
    "dataset = dataset.padded_batch(4, padded_shapes=[None]) # 按一个batch里最长的那句的长度来填充\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(5):\n",
    "        print(sess.run(next_element))  # ==> [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]]\n",
    "#     print(sess.run(next_element))  # ==> [[4, 4, 4, 4, 0, 0, 0],\n",
    "                                   #      [5, 5, 5, 5, 5, 0, 0],\n",
    "                                   #      [6, 6, 6, 6, 6, 6, 0],\n",
    "                                   #      [7, 7, 7, 7, 7, 7, 7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor `a` is [1.8, 2.2], dtype=tf.float\n",
    "tf.cast(a, tf.int32) ==> [1, 2]  # dtype=tf.int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

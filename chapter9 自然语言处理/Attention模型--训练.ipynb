{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wujiaocan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.参数设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设输入数据已经用9.2.1小节中的方法转换成了单词编号的格式。\n",
    "SRC_TRAIN_DATA = \"./data/train.en\"          # 源语言输入文件。\n",
    "TRG_TRAIN_DATA = \"./data/train.zh\"          # 目标语言输入文件。\n",
    "CHECKPOINT_PATH = \"./data/attention_ckpt\"   # checkpoint保存路径。  \n",
    "\n",
    "HIDDEN_SIZE = 1024                     # LSTM的隐藏层规模。\n",
    "DECODER_LAYERS = 3                    # 解码器中LSTM结构的层数。这个例子中编码器固定使用单层的双向LSTM。\n",
    "SRC_VOCAB_SIZE = 10000                 # 源语言词汇表大小。\n",
    "TRG_VOCAB_SIZE = 4000                  # 目标语言词汇表大小。\n",
    "BATCH_SIZE = 100                       # 训练数据batch的大小。\n",
    "NUM_EPOCH = 5                          # 使用训练数据的轮数。\n",
    "KEEP_PROB = 0.8                        # 节点不被dropout的概率。\n",
    "MAX_GRAD_NORM = 5                      # 用于控制梯度膨胀的梯度大小上限。\n",
    "SHARE_EMB_AND_SOFTMAX = True           # 在Softmax层和词向量层之间共享参数。\n",
    "\n",
    "MAX_LEN = 50   # 限定句子的最大单词数量。\n",
    "SOS_ID  = 1    # 目标语言词汇表中<sos>的ID。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.读取训练数据并创建Dataset。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Dataset从一个文件中读取一个语言的数据。\n",
    "# 数据的格式为每行一句话，单词已经转化为单词编号。\n",
    "def MakeDataset(file_path):\n",
    "    dataset = tf.data.TextLineDataset(file_path)\n",
    "    # 根据空格将单词编号切分开并放入一个一维向量。\n",
    "    dataset = dataset.map(lambda string: tf.string_split([string]).values)\n",
    "    # 将字符串形式的单词编号转化为整数。\n",
    "    dataset = dataset.map(\n",
    "        lambda string: tf.string_to_number(string, tf.int32))\n",
    "    # 统计每个句子的单词数量，并与句子内容一起放入Dataset中。\n",
    "    dataset = dataset.map(lambda x: (x, tf.size(x)))\n",
    "    return dataset\n",
    "\n",
    "# 从源语言文件src_path和目标语言文件trg_path中分别读取数据，并进行填充和\n",
    "# batching操作。\n",
    "def MakeSrcTrgDataset(src_path, trg_path, batch_size):\n",
    "    # 首先分别读取源语言数据和目标语言数据。\n",
    "    src_data = MakeDataset(src_path)\n",
    "    trg_data = MakeDataset(trg_path)\n",
    "    # 通过zip操作将两个Dataset合并为一个Dataset。现在每个Dataset中每一项数据ds\n",
    "    # 由4个张量组成：\n",
    "    #   ds[0][0]是源句子\n",
    "    #   ds[0][1]是源句子长度\n",
    "    #   ds[1][0]是目标句子\n",
    "    #   ds[1][1]是目标句子长度\n",
    "    dataset = tf.data.Dataset.zip((src_data, trg_data))\n",
    "\n",
    "    # 删除内容为空（只包含<EOS>）的句子和长度过长的句子。\n",
    "    def FilterLength(src_tuple, trg_tuple):\n",
    "        ((src_input, src_len), (trg_label, trg_len)) = (src_tuple, trg_tuple)\n",
    "        src_len_ok = tf.logical_and(\n",
    "            tf.greater(src_len, 1), tf.less_equal(src_len, MAX_LEN))\n",
    "        trg_len_ok = tf.logical_and(\n",
    "            tf.greater(trg_len, 1), tf.less_equal(trg_len, MAX_LEN))\n",
    "        return tf.logical_and(src_len_ok, trg_len_ok)\n",
    "    dataset = dataset.filter(FilterLength)\n",
    "    \n",
    "    # 从图9-5可知，解码器需要两种格式的目标句子：\n",
    "    #   1.解码器的输入(trg_input)，形式如同\"<sos> X Y Z\"\n",
    "    #   2.解码器的目标输出(trg_label)，形式如同\"X Y Z <eos>\"\n",
    "    # 上面从文件中读到的目标句子是\"X Y Z <eos>\"的形式，我们需要从中生成\"<sos> X Y Z\"\n",
    "    # 形式并加入到Dataset中。\n",
    "    def MakeTrgInput(src_tuple, trg_tuple):\n",
    "        ((src_input, src_len), (trg_label, trg_len)) = (src_tuple, trg_tuple)\n",
    "        trg_input = tf.concat([[SOS_ID], trg_label[:-1]], axis=0)\n",
    "        return ((src_input, src_len), (trg_input, trg_label, trg_len))\n",
    "    dataset = dataset.map(MakeTrgInput)\n",
    "\n",
    "    # 随机打乱训练数据。\n",
    "    dataset = dataset.shuffle(10000)\n",
    "\n",
    "    # 规定填充后输出的数据维度。\n",
    "    padded_shapes = (\n",
    "        (tf.TensorShape([None]),      # 源句子是长度未知的向量\n",
    "         tf.TensorShape([])),         # 源句子长度是单个数字\n",
    "        (tf.TensorShape([None]),      # 目标句子（解码器输入）是长度未知的向量\n",
    "         tf.TensorShape([None]),      # 目标句子（解码器目标输出）是长度未知的向量\n",
    "         tf.TensorShape([])))         # 目标句子长度是单个数字\n",
    "    # 调用padded_batch方法进行batching操作。\n",
    "    batched_dataset = dataset.padded_batch(batch_size, padded_shapes)\n",
    "    return batched_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.定义翻译模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义NMTModel类来描述模型。\n",
    "class NMTModel(object):\n",
    "    # 在模型的初始化函数中定义模型要用到的变量。\n",
    "    def __init__(self):\n",
    "        # 定义编码器和解码器所使用的LSTM结构。\n",
    "        self.enc_cell_fw = tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)\n",
    "        self.enc_cell_bw = tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)\n",
    "        \n",
    "        self.dec_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "          [tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE) for _ in range(DECODER_LAYERS)])\n",
    "\n",
    "        # 为源语言和目标语言分别定义词向量。   \n",
    "        self.src_embedding = tf.get_variable(\n",
    "            \"src_emb\", [SRC_VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        self.trg_embedding = tf.get_variable(\n",
    "            \"trg_emb\", [TRG_VOCAB_SIZE, HIDDEN_SIZE])\n",
    "\n",
    "        # 定义softmax层的变量\n",
    "        if SHARE_EMB_AND_SOFTMAX:\n",
    "           self.softmax_weight = tf.transpose(self.trg_embedding)\n",
    "        else:\n",
    "           self.softmax_weight = tf.get_variable(\n",
    "               \"weight\", [HIDDEN_SIZE, TRG_VOCAB_SIZE])\n",
    "        self.softmax_bias = tf.get_variable(\n",
    "            \"softmax_bias\", [TRG_VOCAB_SIZE])\n",
    "\n",
    "    # 在forward函数中定义模型的前向计算图。\n",
    "    # src_input, src_size, trg_input, trg_label, trg_size分别是上面\n",
    "    # MakeSrcTrgDataset函数产生的五种张量。\n",
    "    def forward(self, src_input, src_size, trg_input, trg_label, trg_size):\n",
    "        batch_size = tf.shape(src_input)[0]\n",
    "    \n",
    "        # 将输入和输出单词编号转为词向量。\n",
    "        src_emb = tf.nn.embedding_lookup(self.src_embedding, src_input)\n",
    "        trg_emb = tf.nn.embedding_lookup(self.trg_embedding, trg_input)\n",
    "        \n",
    "        # 在词向量上进行dropout。\n",
    "        src_emb = tf.nn.dropout(src_emb, KEEP_PROB)\n",
    "        trg_emb = tf.nn.dropout(trg_emb, KEEP_PROB)\n",
    "\n",
    "        # 使用dynamic_rnn构造编码器。\n",
    "        # 编码器读取源句子每个位置的词向量，输出最后一步的隐藏状态enc_state。\n",
    "        # 因为编码器是一个双层LSTM，因此enc_state是一个包含两个LSTMStateTuple类\n",
    "        # 张量的tuple，每个LSTMStateTuple对应编码器中的一层。\n",
    "        # 张量的维度是 [batch_size, HIDDEN_SIZE]。\n",
    "        # enc_outputs是顶层LSTM在每一步的输出，它的维度是[batch_size, \n",
    "        # max_time, HIDDEN_SIZE]。Seq2Seq模型中不需要用到enc_outputs，而\n",
    "        # 后面介绍的attention模型会用到它。\n",
    "        # 下面的代码取代了Seq2Seq样例代码中forward函数里的相应部分。\n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            # 构造编码器时，使用bidirectional_dynamic_rnn构造双向循环网络。\n",
    "            # 双向循环网络的顶层输出enc_outputs是一个包含两个张量的tuple，每个张量的\n",
    "            # 维度都是[batch_size, max_time, HIDDEN_SIZE]，代表两个LSTM在每一步的输出。\n",
    "            enc_outputs, enc_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "                self.enc_cell_fw, self.enc_cell_bw, src_emb, src_size, \n",
    "                dtype=tf.float32)\n",
    "            # 将两个LSTM的输出拼接为一个张量。\n",
    "            enc_outputs = tf.concat([enc_outputs[0], enc_outputs[1]], -1)     \n",
    "\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            # 选择注意力权重的计算模型。BahdanauAttention是使用一个隐藏层的前馈神经网络。\n",
    "            # memory_sequence_length是一个维度为[batch_size]的张量，代表batch\n",
    "            # 中每个句子的长度，Attention需要根据这个信息把填充位置的注意力权重设置为0。\n",
    "            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                HIDDEN_SIZE, enc_outputs,\n",
    "                memory_sequence_length=src_size)\n",
    "\n",
    "            # 将解码器的循环神经网络self.dec_cell和注意力一起封装成更高层的循环神经网络。\n",
    "            attention_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                self.dec_cell, attention_mechanism,\n",
    "                attention_layer_size=HIDDEN_SIZE)\n",
    "\n",
    "            # 使用attention_cell和dynamic_rnn构造编码器。\n",
    "            # 这里没有指定init_state，也就是没有使用编码器的输出来初始化输入，而完全依赖\n",
    "            # 注意力作为信息来源。\n",
    "            dec_outputs, _ = tf.nn.dynamic_rnn(\n",
    "                attention_cell, trg_emb, trg_size, dtype=tf.float32)\n",
    "\n",
    "        # 计算解码器每一步的log perplexity。这一步与语言模型代码相同。\n",
    "        output = tf.reshape(dec_outputs, [-1, HIDDEN_SIZE])\n",
    "        logits = tf.matmul(output, self.softmax_weight) + self.softmax_bias\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=tf.reshape(trg_label, [-1]), logits=logits)\n",
    "\n",
    "        # 在计算平均损失时，需要将填充位置的权重设置为0，以避免无效位置的预测干扰\n",
    "        # 模型的训练。\n",
    "        label_weights = tf.sequence_mask(\n",
    "            trg_size, maxlen=tf.shape(trg_label)[1], dtype=tf.float32)\n",
    "        label_weights = tf.reshape(label_weights, [-1])\n",
    "        cost = tf.reduce_sum(loss * label_weights)\n",
    "        cost_per_token = cost / tf.reduce_sum(label_weights)\n",
    "        \n",
    "        # 定义反向传播操作。反向操作的实现与语言模型代码相同。\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "\n",
    "        # 控制梯度大小，定义优化方法和训练步骤。\n",
    "        grads = tf.gradients(cost / tf.to_float(batch_size),\n",
    "                             trainable_variables)\n",
    "        grads, _ = tf.clip_by_global_norm(grads, MAX_GRAD_NORM)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables))\n",
    "        return cost_per_token, train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.训练过程和主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In iteration: 1\n",
      "After 0 steps, per token cost is 8.290\n",
      "After 10 steps, per token cost is 7.287\n",
      "After 20 steps, per token cost is 6.833\n",
      "After 30 steps, per token cost is 6.574\n",
      "After 40 steps, per token cost is 6.748\n",
      "After 50 steps, per token cost is 6.571\n",
      "After 60 steps, per token cost is 6.465\n",
      "After 70 steps, per token cost is 6.419\n",
      "After 80 steps, per token cost is 6.392\n",
      "After 90 steps, per token cost is 6.253\n",
      "After 100 steps, per token cost is 6.608\n",
      "After 110 steps, per token cost is 6.195\n",
      "After 120 steps, per token cost is 6.452\n",
      "After 130 steps, per token cost is 6.282\n",
      "After 140 steps, per token cost is 6.247\n",
      "After 150 steps, per token cost is 6.173\n",
      "After 160 steps, per token cost is 6.220\n",
      "After 170 steps, per token cost is 6.268\n",
      "After 180 steps, per token cost is 6.238\n",
      "After 190 steps, per token cost is 6.171\n",
      "After 200 steps, per token cost is 6.203\n",
      "After 210 steps, per token cost is 6.250\n",
      "After 220 steps, per token cost is 6.069\n",
      "After 230 steps, per token cost is 6.102\n",
      "After 240 steps, per token cost is 6.124\n",
      "After 250 steps, per token cost is 6.071\n",
      "After 260 steps, per token cost is 6.179\n",
      "After 270 steps, per token cost is 6.220\n",
      "After 280 steps, per token cost is 6.103\n",
      "After 290 steps, per token cost is 6.163\n",
      "After 300 steps, per token cost is 6.102\n",
      "After 310 steps, per token cost is 6.103\n",
      "After 320 steps, per token cost is 6.103\n",
      "After 330 steps, per token cost is 6.200\n",
      "After 340 steps, per token cost is 6.122\n",
      "After 350 steps, per token cost is 6.063\n",
      "After 360 steps, per token cost is 6.081\n",
      "After 370 steps, per token cost is 6.031\n",
      "After 380 steps, per token cost is 6.142\n",
      "After 390 steps, per token cost is 6.140\n",
      "After 400 steps, per token cost is 6.153\n",
      "After 410 steps, per token cost is 6.201\n",
      "After 420 steps, per token cost is 6.164\n",
      "After 430 steps, per token cost is 6.135\n",
      "After 440 steps, per token cost is 6.139\n",
      "After 450 steps, per token cost is 6.046\n",
      "After 460 steps, per token cost is 6.074\n",
      "After 470 steps, per token cost is 6.123\n",
      "After 480 steps, per token cost is 6.124\n",
      "After 490 steps, per token cost is 6.102\n",
      "After 500 steps, per token cost is 6.100\n",
      "After 510 steps, per token cost is 6.219\n",
      "After 520 steps, per token cost is 6.003\n",
      "After 530 steps, per token cost is 5.999\n",
      "After 540 steps, per token cost is 6.124\n",
      "After 550 steps, per token cost is 6.059\n",
      "After 560 steps, per token cost is 6.123\n",
      "In iteration: 2\n",
      "After 570 steps, per token cost is 6.062\n",
      "After 580 steps, per token cost is 6.113\n",
      "After 590 steps, per token cost is 6.045\n",
      "After 600 steps, per token cost is 6.115\n",
      "After 610 steps, per token cost is 6.133\n",
      "After 620 steps, per token cost is 6.080\n",
      "After 630 steps, per token cost is 6.104\n",
      "After 640 steps, per token cost is 6.140\n",
      "After 650 steps, per token cost is 6.019\n",
      "After 660 steps, per token cost is 6.075\n",
      "After 670 steps, per token cost is 6.023\n",
      "After 680 steps, per token cost is 6.111\n",
      "After 690 steps, per token cost is 6.007\n",
      "After 700 steps, per token cost is 6.058\n",
      "After 710 steps, per token cost is 6.153\n",
      "After 720 steps, per token cost is 6.092\n",
      "After 730 steps, per token cost is 5.951\n",
      "After 740 steps, per token cost is 5.959\n",
      "After 750 steps, per token cost is 6.066\n",
      "After 760 steps, per token cost is 6.027\n",
      "After 770 steps, per token cost is 6.020\n",
      "After 780 steps, per token cost is 5.988\n",
      "After 790 steps, per token cost is 5.963\n",
      "After 800 steps, per token cost is 5.937\n",
      "After 810 steps, per token cost is 6.053\n",
      "After 820 steps, per token cost is 5.987\n",
      "After 830 steps, per token cost is 6.082\n",
      "After 840 steps, per token cost is 6.063\n",
      "After 850 steps, per token cost is 6.106\n",
      "After 860 steps, per token cost is 6.046\n",
      "After 870 steps, per token cost is 6.042\n",
      "After 880 steps, per token cost is 5.970\n",
      "After 890 steps, per token cost is 6.054\n",
      "After 900 steps, per token cost is 6.014\n",
      "After 910 steps, per token cost is 5.977\n",
      "After 920 steps, per token cost is 6.035\n",
      "After 930 steps, per token cost is 6.064\n",
      "After 940 steps, per token cost is 5.902\n",
      "After 950 steps, per token cost is 5.994\n",
      "After 960 steps, per token cost is 6.013\n",
      "After 970 steps, per token cost is 5.897\n",
      "After 980 steps, per token cost is 6.030\n",
      "After 990 steps, per token cost is 6.057\n",
      "After 1000 steps, per token cost is 5.914\n",
      "After 1010 steps, per token cost is 6.077\n",
      "After 1020 steps, per token cost is 6.021\n",
      "After 1030 steps, per token cost is 6.012\n",
      "After 1040 steps, per token cost is 6.082\n",
      "After 1050 steps, per token cost is 5.980\n",
      "After 1060 steps, per token cost is 5.995\n",
      "After 1070 steps, per token cost is 6.073\n",
      "After 1080 steps, per token cost is 6.054\n",
      "After 1090 steps, per token cost is 6.079\n",
      "After 1100 steps, per token cost is 6.121\n",
      "After 1110 steps, per token cost is 6.025\n",
      "After 1120 steps, per token cost is 6.095\n",
      "In iteration: 3\n",
      "After 1130 steps, per token cost is 5.996\n",
      "After 1140 steps, per token cost is 6.123\n",
      "After 1150 steps, per token cost is 5.975\n",
      "After 1160 steps, per token cost is 5.954\n",
      "After 1170 steps, per token cost is 5.983\n",
      "After 1180 steps, per token cost is 6.089\n",
      "After 1190 steps, per token cost is 5.928\n",
      "After 1200 steps, per token cost is 6.059\n",
      "After 1210 steps, per token cost is 6.117\n",
      "After 1220 steps, per token cost is 6.075\n",
      "After 1230 steps, per token cost is 5.997\n",
      "After 1240 steps, per token cost is 5.976\n",
      "After 1250 steps, per token cost is 6.026\n",
      "After 1260 steps, per token cost is 5.987\n",
      "After 1270 steps, per token cost is 5.998\n",
      "After 1280 steps, per token cost is 5.921\n",
      "After 1290 steps, per token cost is 5.850\n",
      "After 1300 steps, per token cost is 6.020\n",
      "After 1310 steps, per token cost is 6.006\n",
      "After 1320 steps, per token cost is 5.930\n",
      "After 1330 steps, per token cost is 5.941\n",
      "After 1340 steps, per token cost is 5.953\n",
      "After 1350 steps, per token cost is 5.898\n",
      "After 1360 steps, per token cost is 5.847\n",
      "After 1370 steps, per token cost is 5.907\n",
      "After 1380 steps, per token cost is 5.898\n",
      "After 1390 steps, per token cost is 5.923\n",
      "After 1400 steps, per token cost is 5.880\n",
      "After 1410 steps, per token cost is 5.975\n",
      "After 1420 steps, per token cost is 5.955\n",
      "After 1430 steps, per token cost is 5.911\n",
      "After 1440 steps, per token cost is 5.965\n",
      "After 1450 steps, per token cost is 5.948\n",
      "After 1460 steps, per token cost is 6.067\n",
      "After 1470 steps, per token cost is 5.918\n",
      "After 1480 steps, per token cost is 5.983\n",
      "After 1490 steps, per token cost is 6.017\n",
      "After 1500 steps, per token cost is 5.912\n",
      "After 1510 steps, per token cost is 5.916\n",
      "After 1520 steps, per token cost is 6.001\n",
      "After 1530 steps, per token cost is 5.953\n",
      "After 1540 steps, per token cost is 5.946\n",
      "After 1550 steps, per token cost is 5.907\n",
      "After 1560 steps, per token cost is 5.857\n",
      "After 1570 steps, per token cost is 6.040\n",
      "After 1580 steps, per token cost is 5.896\n",
      "After 1590 steps, per token cost is 5.900\n",
      "After 1600 steps, per token cost is 6.026\n",
      "After 1610 steps, per token cost is 5.838\n",
      "After 1620 steps, per token cost is 5.907\n",
      "After 1630 steps, per token cost is 5.957\n",
      "After 1640 steps, per token cost is 5.891\n",
      "After 1650 steps, per token cost is 5.934\n",
      "After 1660 steps, per token cost is 5.858\n",
      "After 1670 steps, per token cost is 6.001\n",
      "After 1680 steps, per token cost is 5.997\n",
      "After 1690 steps, per token cost is 5.936\n",
      "In iteration: 4\n",
      "After 1700 steps, per token cost is 5.948\n",
      "After 1710 steps, per token cost is 5.839\n",
      "After 1720 steps, per token cost is 5.965\n",
      "After 1730 steps, per token cost is 5.852\n",
      "After 1740 steps, per token cost is 5.938\n",
      "After 1750 steps, per token cost is 5.983\n",
      "After 1760 steps, per token cost is 5.959\n",
      "After 1770 steps, per token cost is 5.886\n",
      "After 1780 steps, per token cost is 5.877\n",
      "After 1790 steps, per token cost is 5.932\n",
      "After 1800 steps, per token cost is 5.788\n",
      "After 1810 steps, per token cost is 6.004\n",
      "After 1820 steps, per token cost is 6.047\n",
      "After 1830 steps, per token cost is 5.815\n",
      "After 1840 steps, per token cost is 5.847\n",
      "After 1850 steps, per token cost is 5.900\n",
      "After 1860 steps, per token cost is 5.921\n",
      "After 1870 steps, per token cost is 5.926\n",
      "After 1880 steps, per token cost is 5.868\n",
      "After 1890 steps, per token cost is 5.906\n",
      "After 1900 steps, per token cost is 5.880\n",
      "After 1910 steps, per token cost is 5.920\n",
      "After 1920 steps, per token cost is 5.832\n",
      "After 1930 steps, per token cost is 5.782\n",
      "After 1940 steps, per token cost is 5.896\n",
      "After 1950 steps, per token cost is 5.784\n",
      "After 1960 steps, per token cost is 5.848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1970 steps, per token cost is 5.953\n",
      "After 1980 steps, per token cost is 5.864\n",
      "After 1990 steps, per token cost is 5.838\n",
      "After 2000 steps, per token cost is 5.783\n",
      "After 2010 steps, per token cost is 5.898\n",
      "After 2020 steps, per token cost is 5.803\n",
      "After 2030 steps, per token cost is 5.961\n",
      "After 2040 steps, per token cost is 5.820\n",
      "After 2050 steps, per token cost is 5.757\n",
      "After 2060 steps, per token cost is 5.756\n",
      "After 2070 steps, per token cost is 5.911\n",
      "After 2080 steps, per token cost is 5.888\n",
      "After 2090 steps, per token cost is 5.925\n",
      "After 2100 steps, per token cost is 5.785\n",
      "After 2110 steps, per token cost is 5.964\n",
      "After 2120 steps, per token cost is 5.852\n",
      "After 2130 steps, per token cost is 5.777\n",
      "After 2140 steps, per token cost is 5.790\n",
      "After 2150 steps, per token cost is 5.814\n",
      "After 2160 steps, per token cost is 5.745\n",
      "After 2170 steps, per token cost is 5.866\n",
      "After 2180 steps, per token cost is 5.824\n",
      "After 2190 steps, per token cost is 5.803\n",
      "After 2200 steps, per token cost is 5.934\n",
      "After 2210 steps, per token cost is 5.760\n",
      "After 2220 steps, per token cost is 5.790\n",
      "After 2230 steps, per token cost is 5.869\n",
      "After 2240 steps, per token cost is 5.824\n",
      "After 2250 steps, per token cost is 5.763\n",
      "In iteration: 5\n",
      "After 2260 steps, per token cost is 5.849\n",
      "After 2270 steps, per token cost is 5.841\n",
      "After 2280 steps, per token cost is 5.743\n",
      "After 2290 steps, per token cost is 5.753\n",
      "After 2300 steps, per token cost is 5.730\n",
      "After 2310 steps, per token cost is 5.877\n",
      "After 2320 steps, per token cost is 5.775\n",
      "After 2330 steps, per token cost is 5.800\n",
      "After 2340 steps, per token cost is 5.747\n",
      "After 2350 steps, per token cost is 5.735\n",
      "After 2360 steps, per token cost is 5.871\n",
      "After 2370 steps, per token cost is 5.846\n",
      "After 2380 steps, per token cost is 5.698\n",
      "After 2390 steps, per token cost is 5.798\n",
      "After 2400 steps, per token cost is 5.858\n",
      "After 2410 steps, per token cost is 5.812\n",
      "After 2420 steps, per token cost is 5.855\n",
      "After 2430 steps, per token cost is 5.813\n",
      "After 2440 steps, per token cost is 5.813\n",
      "After 2450 steps, per token cost is 5.817\n",
      "After 2460 steps, per token cost is 5.782\n",
      "After 2470 steps, per token cost is 5.883\n",
      "After 2480 steps, per token cost is 5.752\n",
      "After 2490 steps, per token cost is 5.847\n",
      "After 2500 steps, per token cost is 5.861\n",
      "After 2510 steps, per token cost is 5.793\n",
      "After 2520 steps, per token cost is 5.727\n",
      "After 2530 steps, per token cost is 5.667\n",
      "After 2540 steps, per token cost is 5.712\n",
      "After 2550 steps, per token cost is 5.743\n",
      "After 2560 steps, per token cost is 5.754\n",
      "After 2570 steps, per token cost is 5.845\n",
      "After 2580 steps, per token cost is 5.754\n",
      "After 2590 steps, per token cost is 5.752\n",
      "After 2600 steps, per token cost is 5.634\n",
      "After 2610 steps, per token cost is 5.713\n",
      "After 2620 steps, per token cost is 5.842\n",
      "After 2630 steps, per token cost is 5.746\n",
      "After 2640 steps, per token cost is 5.702\n",
      "After 2650 steps, per token cost is 5.786\n",
      "After 2660 steps, per token cost is 5.788\n",
      "After 2670 steps, per token cost is 5.879\n",
      "After 2680 steps, per token cost is 5.762\n",
      "After 2690 steps, per token cost is 5.758\n",
      "After 2700 steps, per token cost is 5.831\n",
      "After 2710 steps, per token cost is 5.713\n",
      "After 2720 steps, per token cost is 5.754\n",
      "After 2730 steps, per token cost is 5.673\n",
      "After 2740 steps, per token cost is 5.809\n",
      "After 2750 steps, per token cost is 5.753\n",
      "After 2760 steps, per token cost is 5.806\n",
      "After 2770 steps, per token cost is 5.665\n",
      "After 2780 steps, per token cost is 5.753\n",
      "After 2790 steps, per token cost is 5.799\n",
      "After 2800 steps, per token cost is 5.696\n",
      "After 2810 steps, per token cost is 5.734\n",
      "After 2820 steps, per token cost is 5.730\n"
     ]
    }
   ],
   "source": [
    "# 使用给定的模型model上训练一个epoch，并返回全局步数。\n",
    "# 每训练200步便保存一个checkpoint。\n",
    "def run_epoch(session, cost_op, train_op, saver, step):\n",
    "    # 训练一个epoch。\n",
    "    # 重复训练步骤直至遍历完Dataset中所有数据。\n",
    "    while True:\n",
    "        try:\n",
    "            # 运行train_op并计算损失值。训练数据在main()函数中以Dataset方式提供。\n",
    "            cost, _ = session.run([cost_op, train_op])\n",
    "            if step % 10 == 0:\n",
    "                print(\"After %d steps, per token cost is %.3f\" % (step, cost))\n",
    "            # 每200步保存一个checkpoint。\n",
    "            if step % 200 == 0:\n",
    "                saver.save(session, CHECKPOINT_PATH, global_step=step)\n",
    "            step += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    return step\n",
    "\n",
    "def main():\n",
    "    # 定义初始化函数。\n",
    "    initializer = tf.random_uniform_initializer(-0.05, 0.05)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # 定义训练用的循环神经网络模型。\n",
    "    with tf.variable_scope(\"nmt_model\", reuse=None, \n",
    "                           initializer=initializer):\n",
    "        train_model = NMTModel()\n",
    "  \n",
    "    # 定义输入数据。\n",
    "    data = MakeSrcTrgDataset(SRC_TRAIN_DATA, TRG_TRAIN_DATA, BATCH_SIZE)\n",
    "    iterator = data.make_initializable_iterator()\n",
    "    (src, src_size), (trg_input, trg_label, trg_size) = iterator.get_next()\n",
    " \n",
    "    # 定义前向计算图。输入数据以张量形式提供给forward函数。\n",
    "    cost_op, train_op = train_model.forward(src, src_size, trg_input,\n",
    "                                            trg_label, trg_size)\n",
    "\n",
    "    # 训练模型。\n",
    "    saver = tf.train.Saver()\n",
    "    step = 0\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for i in range(NUM_EPOCH):\n",
    "            print(\"In iteration: %d\" % (i + 1))\n",
    "            sess.run(iterator.initializer)\n",
    "            step = run_epoch(sess, cost_op, train_op, saver, step)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

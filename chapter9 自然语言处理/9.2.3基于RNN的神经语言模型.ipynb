{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wujiaocan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = \"./data/ptb.train\"          # 训练数据路径。\n",
    "EVAL_DATA = \"./data/ptb.valid\"           # 验证数据路径。\n",
    "TEST_DATA = \"./data/ptb.test\"            # 测试数据路径。\n",
    "HIDDEN_SIZE = 300                 # 隐藏层规模。\n",
    "NUM_LAYERS = 2                    # 深层循环神经网络中LSTM结构的层数。\n",
    "VOCAB_SIZE = 10000                # 词典规模。\n",
    "TRAIN_BATCH_SIZE = 20             # 训练数据batch的大小。\n",
    "TRAIN_NUM_STEP = 35               # 训练数据截断长度。\n",
    "\n",
    "EVAL_BATCH_SIZE = 1               # 测试数据batch的大小。\n",
    "EVAL_NUM_STEP = 1                 # 测试数据截断长度。\n",
    "NUM_EPOCH = 5                     # 使用训练数据的轮数。\n",
    "LSTM_KEEP_PROB = 0.9              # LSTM节点不被dropout的概率。\n",
    "EMBEDDING_KEEP_PROB = 0.9         # 词向量不被dropout的概率。\n",
    "MAX_GRAD_NORM = 5                 # 用于控制梯度膨胀的梯度大小上限。\n",
    "SHARE_EMB_AND_SOFTMAX = True      # 在Softmax层和词向量层之间共享参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过一个PTBModel类来描述模型，这样方便维护循环神经网络中的状态。\n",
    "class PTBModel(object):\n",
    "    def __init__(self, is_training, batch_size, num_steps):\n",
    "        # 记录使用的batch大小和截断长度。\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        \n",
    "        # 定义每一步的输入和预期输出。两者的维度都是[batch_size, num_steps]。\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        \n",
    "        # 定义使用LSTM结构为循环体结构且使用dropout的深层循环神经网络。\n",
    "        dropout_keep_prob = LSTM_KEEP_PROB if is_training else 1.0\n",
    "        lstm_cells = [\n",
    "            tf.nn.rnn_cell.DropoutWrapper(\n",
    "                tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE),\n",
    "                output_keep_prob=dropout_keep_prob)\n",
    "            for _ in range(NUM_LAYERS)]     \n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)            \n",
    "        \n",
    "        # 初始化最初的状态，即全零的向量。这个量只在每个epoch初始化第一个batch\n",
    "        # 时使用。\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        # 定义单词的词向量矩阵。\n",
    "        embedding = tf.get_variable(\"embedding\", [VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        \n",
    "        # 将输入单词转化为词向量。\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "        \n",
    "        # 只在训练时使用dropout。\n",
    "        if is_training:\n",
    "            inputs = tf.nn.dropout(inputs, EMBEDDING_KEEP_PROB)\n",
    " \n",
    "        # 定义输出列表。在这里先将不同时刻LSTM结构的输出收集起来，再一起提供给\n",
    "        # softmax层。\n",
    "        outputs = []\n",
    "        state = self.initial_state\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "                cell_output, state = cell(inputs[:, time_step, :], state)\n",
    "                outputs.append(cell_output) \n",
    "        # 把输出队列展开成[batch, hidden_size*num_steps]的形状，然后再\n",
    "        # reshape成[batch*numsteps, hidden_size]的形状。\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, HIDDEN_SIZE])\n",
    " \n",
    "        # Softmax层：将RNN在每个位置上的输出转化为各个单词的logits。\n",
    "        if SHARE_EMB_AND_SOFTMAX:\n",
    "            weight = tf.transpose(embedding)\n",
    "        else:\n",
    "            weight = tf.get_variable(\"weight\", [HIDDEN_SIZE, VOCAB_SIZE])\n",
    "        bias = tf.get_variable(\"bias\", [VOCAB_SIZE])\n",
    "        logits = tf.matmul(output, weight) + bias\n",
    "        \n",
    "        # 定义交叉熵损失函数和平均损失。\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=tf.reshape(self.targets, [-1]),\n",
    "            logits=logits)\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size\n",
    "        self.final_state = state\n",
    "        \n",
    "        # 只在训练模型时定义反向传播操作。\n",
    "        if not is_training: return\n",
    "\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        # 控制梯度大小，定义优化方法和训练步骤。\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    def __init__(self, is_training, batch_size, num_steps):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        dropout_keep_prob = LSTM_KEEP_PROB if is_training else 1.0\n",
    "        lstm_cells = [\n",
    "            tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE),output_keep_prob=dropout_keep_prob)\n",
    "            for _ in range(NUM_LAYERS)\n",
    "        ]\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)  # state维度是[batch_size, hidden_size]\n",
    "        embedding = tf.get_variable(\"embedding\", [VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "        if is_training:\n",
    "            inputs = tf.nn.dropout(inputs, EMBEDDING_KEEP_PROB)\n",
    "        outputs = []\n",
    "        state = self.initial_state\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "                cell_output, state = cell(inputs[:, time_step, :], state)\n",
    "                outputs.append(cell_output)\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, HIDDEN_SIZE])\n",
    "        if SHARE_EMB_AND_SOFTMAX:\n",
    "            weight = tf.transpose(embedding)\n",
    "        else:\n",
    "            weight = tf.get_variable(\"weight\", [HIDDEN_SIZE, VOCAN_SIZE])\n",
    "        bias = tf.get_variable(\"bias\", [COVAB_SIZE])\n",
    "        logits = tf.matmul(output, weight) + bias\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels = tf.reshape(self.targets, [-1]),\n",
    "            logits = logits)\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size\n",
    "        self.final_state = state\n",
    "        if not is_training: return\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用给定的模型model在数据data上运行train_op并返回在全部数据上的perplexity值。\n",
    "def run_epoch(session, model, batches, train_op, output_log, step):\n",
    "    # 计算平均perplexity的辅助变量。\n",
    "    total_costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state) \n",
    "    # 训练一个epoch。\n",
    "    for x, y in batches:\n",
    "        # 在当前batch上运行train_op并计算损失值。交叉熵损失函数计算的就是下一个单\n",
    "        # 词为给定单词的概率。\n",
    "        cost, state, _ = session.run(\n",
    "             [model.cost, model.final_state, train_op],\n",
    "             {model.input_data: x, model.targets: y,\n",
    "              model.initial_state: state})\n",
    "        total_costs += cost\n",
    "        iters += model.num_steps\n",
    "\n",
    "        # 只有在训练时输出日志。\n",
    "        if output_log and step % 100 == 0:\n",
    "            print(\"After %d steps, perplexity is %.3f\" % (\n",
    "                  step, np.exp(total_costs / iters)))\n",
    "        step += 1\n",
    "\n",
    "    # 返回给定模型在给定数据上的perplexity值。\n",
    "    return step, np.exp(total_costs / iters)\n",
    "\n",
    "\n",
    "# 从文件中读取数据，并返回包含单词编号的数据\n",
    "def read_data(file_path):\n",
    "    with open(file_path, \"r\") as fin:\n",
    "        # 将整个文档读进一个长字符串\n",
    "        id_string=\" \".join([line.strip() for line in fin.readlines()])\n",
    "#         print(id_string[:1000])\n",
    "    id_list = [round(int(w)) for w in id_string.split()]  # 将读取的单词编号转为整数\n",
    "    return id_list\n",
    "\n",
    "\n",
    "def make_batches(id_list, batch_size, num_step):\n",
    "    # 计算总的batch数量。每个batch包含的单词数量是batch_size * num_step。\n",
    "    num_batches = (len(id_list) - 1) // (batch_size * num_step)\n",
    "\n",
    "    # 如9-4图所示，将数据整理成一个维度为[batch_size, num_batches * num_step]\n",
    "    # 的二维数组。\n",
    "    data = np.array(id_list[: num_batches * batch_size * num_step])\n",
    "    data = np.reshape(data, [batch_size, num_batches * num_step])\n",
    "    # 沿着第二个维度将数据切分成num_batches个batch，存入一个数组。\n",
    "    data_batches = np.split(data, num_batches, axis=1)\n",
    "\n",
    "    # 重复上述操作，但是每个位置向右移动一位。这里得到的是RNN每一步输出所需要预测的\n",
    "    # 下一个单词。\n",
    "    label = np.array(id_list[1 : num_batches * batch_size * num_step + 1]) \n",
    "    label = np.reshape(label, [batch_size, num_batches * num_step])\n",
    "    label_batches = np.split(label, num_batches, axis=1)  \n",
    "    # 返回一个长度为num_batches的数组，其中每一项包括一个data矩阵和一个label矩阵。\n",
    "    return list(zip(data_batches, label_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(session, model, batches, train_op, output_log, step):\n",
    "    total_costs = 0.0\n",
    "    iters=0\n",
    "    state = session.run(model.initial_state)\n",
    "    for x, y in batches:\n",
    "        cost, state, _ = session.run(\n",
    "            [model.cost, model.final_state, train_op],{model.input_data:x, model.targets:y, model.initial_state:state})\n",
    "        total_costs += cost\n",
    "        iters += model.num_steps\n",
    "        if output_log and step%100 == 0:\n",
    "            print('After %d steps, perplexity is % 3f' % (step, np.exp(total_costs / iters)))\n",
    "        step += 1\n",
    "    return step, np.exp(total_costs/iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In iteration: 1\n",
      "After 0 steps, perplexity is 9953.181\n",
      "After 100 steps, perplexity is 1678.354\n",
      "After 200 steps, perplexity is 1150.590\n",
      "After 300 steps, perplexity is 914.252\n",
      "After 400 steps, perplexity is 754.553\n",
      "After 500 steps, perplexity is 644.858\n",
      "After 600 steps, perplexity is 571.105\n",
      "After 700 steps, perplexity is 513.926\n",
      "After 800 steps, perplexity is 463.613\n",
      "After 900 steps, perplexity is 426.506\n",
      "After 1000 steps, perplexity is 399.915\n",
      "After 1100 steps, perplexity is 372.992\n",
      "After 1200 steps, perplexity is 351.987\n",
      "After 1300 steps, perplexity is 331.924\n",
      "Epoch: 1 Train Perplexity: 328.737\n",
      "Epoch: 1 Eval Perplexity: 186.410\n",
      "In iteration: 2\n",
      "After 1400 steps, perplexity is 179.003\n",
      "After 1500 steps, perplexity is 164.851\n",
      "After 1600 steps, perplexity is 167.230\n",
      "After 1700 steps, perplexity is 164.464\n",
      "After 1800 steps, perplexity is 159.862\n",
      "After 1900 steps, perplexity is 157.644\n",
      "After 2000 steps, perplexity is 155.987\n",
      "After 2100 steps, perplexity is 151.238\n",
      "After 2200 steps, perplexity is 148.262\n",
      "After 2300 steps, perplexity is 147.044\n",
      "After 2400 steps, perplexity is 144.678\n",
      "After 2500 steps, perplexity is 141.831\n",
      "After 2600 steps, perplexity is 138.388\n",
      "Epoch: 2 Train Perplexity: 137.821\n",
      "Epoch: 2 Eval Perplexity: 135.123\n",
      "In iteration: 3\n",
      "After 2700 steps, perplexity is 122.420\n",
      "After 2800 steps, perplexity is 107.061\n",
      "After 2900 steps, perplexity is 113.639\n",
      "After 3000 steps, perplexity is 111.578\n",
      "After 3100 steps, perplexity is 110.493\n",
      "After 3200 steps, perplexity is 110.496\n",
      "After 3300 steps, perplexity is 110.019\n",
      "After 3400 steps, perplexity is 108.070\n",
      "After 3500 steps, perplexity is 106.167\n",
      "After 3600 steps, perplexity is 105.729\n",
      "After 3700 steps, perplexity is 105.629\n",
      "After 3800 steps, perplexity is 103.660\n",
      "After 3900 steps, perplexity is 101.740\n",
      "Epoch: 3 Train Perplexity: 101.368\n",
      "Epoch: 3 Eval Perplexity: 117.342\n",
      "In iteration: 4\n",
      "After 4000 steps, perplexity is 100.724\n",
      "After 4100 steps, perplexity is 85.472\n",
      "After 4200 steps, perplexity is 90.652\n",
      "After 4300 steps, perplexity is 90.503\n",
      "After 4400 steps, perplexity is 89.605\n",
      "After 4500 steps, perplexity is 89.123\n",
      "After 4600 steps, perplexity is 88.873\n",
      "After 4700 steps, perplexity is 88.082\n",
      "After 4800 steps, perplexity is 86.787\n",
      "After 4900 steps, perplexity is 86.371\n",
      "After 5000 steps, perplexity is 86.683\n",
      "After 5100 steps, perplexity is 85.332\n",
      "After 5200 steps, perplexity is 84.400\n",
      "After 5300 steps, perplexity is 83.959\n",
      "Epoch: 4 Train Perplexity: 83.931\n",
      "Epoch: 4 Eval Perplexity: 110.613\n",
      "In iteration: 5\n",
      "After 5400 steps, perplexity is 74.885\n",
      "After 5500 steps, perplexity is 76.224\n",
      "After 5600 steps, perplexity is 79.458\n",
      "After 5700 steps, perplexity is 77.289\n",
      "After 5800 steps, perplexity is 76.208\n",
      "After 5900 steps, perplexity is 76.501\n",
      "After 6000 steps, perplexity is 76.600\n",
      "After 6100 steps, perplexity is 75.331\n",
      "After 6200 steps, perplexity is 75.041\n",
      "After 6300 steps, perplexity is 75.533\n",
      "After 6400 steps, perplexity is 74.902\n",
      "After 6500 steps, perplexity is 74.161\n",
      "After 6600 steps, perplexity is 73.251\n",
      "Epoch: 5 Train Perplexity: 73.412\n",
      "Epoch: 5 Eval Perplexity: 107.303\n",
      "Test Perplexity: 104.808\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    tf.reset_default_graph()\n",
    "    # 定义初始化函数。\n",
    "    initializer = tf.random_uniform_initializer(-0.05, 0.05)\n",
    "    \n",
    "    # 定义训练用的循环神经网络模型。\n",
    "    with tf.variable_scope(\"language_model\", \n",
    "                           reuse=None, initializer=initializer):\n",
    "        train_model = PTBModel(True, TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)\n",
    "\n",
    "    # 定义测试用的循环神经网络模型。它与train_model共用参数，但是没有dropout。\n",
    "    with tf.variable_scope(\"language_model\",\n",
    "                           reuse=True, initializer=initializer):\n",
    "        eval_model = PTBModel(False, EVAL_BATCH_SIZE, EVAL_NUM_STEP)\n",
    "\n",
    "    # 训练模型。\n",
    "    with tf.Session() as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        train_batches = make_batches(\n",
    "            read_data(TRAIN_DATA), TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)\n",
    "        eval_batches = make_batches(\n",
    "            read_data(EVAL_DATA), EVAL_BATCH_SIZE, EVAL_NUM_STEP)\n",
    "        test_batches = make_batches(\n",
    "            read_data(TEST_DATA), EVAL_BATCH_SIZE, EVAL_NUM_STEP)\n",
    "\n",
    "        step = 0\n",
    "        for i in range(NUM_EPOCH):\n",
    "            print(\"In iteration: %d\" % (i + 1))\n",
    "            step, train_pplx = run_epoch(session, train_model, train_batches,\n",
    "                                         train_model.train_op, True, step)\n",
    "            print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_pplx))\n",
    "\n",
    "            _, eval_pplx = run_epoch(session, eval_model, eval_batches,\n",
    "                                     tf.no_op(), False, 0)\n",
    "            print(\"Epoch: %d Eval Perplexity: %.3f\" % (i + 1, eval_pplx))\n",
    "\n",
    "        _, test_pplx = run_epoch(session, eval_model, test_batches,\n",
    "                                 tf.no_op(), False, 0)\n",
    "        print(\"Test Perplexity: %.3f\" % test_pplx)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  1  1  3  3  3  3]\n",
      " [ 2  2  2  2  4  4  4  4]\n",
      " [ 9  9  9  9 10 10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "t1 = [[[1, 1, 1,1],[2, 2, 2,2],[9,9,9,9]],[[3, 3, 3,3],[4, 4, 4,4],[10,10,10,10]]] \n",
    "t2 = [[[5, 5, 5],[6, 6, 6]],[[7, 7, 7],[8, 8, 8]]] \n",
    "with tf.Session() as sess:    \n",
    "    print(sess.run(tf.concat(t1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = [[5, 5, 5],[6, 6, 6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(6,) dtype=int32>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reshape(t3, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_1:0' shape=(3, 2) dtype=int32>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reshape(t3, [3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经语言模型中，p(Wi|W1,...,Wi-1)分布由一个softmax层产生\n",
    "### tensorflow提供两个函数计算交叉熵：\n",
    "### tf.nn.softmax_cross_entropy_with_logits和tf.nn.sparse_softmax_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wujiaocan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.32656264, 0.4643688 ], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词汇表大小为3，语料包含两个单词“2 0”\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "word_labels = tf.constant([2,0])\n",
    "\n",
    "predict_logits = tf.constant([[2.0, -1.0, 3.0],\n",
    "                             [1.0, 0.0, -0.5]])\n",
    "\n",
    "# 使用tf.nn.sparse_softmax_cross_entropy_with_logits计算交叉熵\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=word_labels, logits=predict_logits)\n",
    "sess = tf.Session()\n",
    "sess.run(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32656264, 0.4643688 ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_prob_distribution = tf.constant([[0.0, 0.0, 1.0],\n",
    "                                     [1.0, 0.0, 0.0]])\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=word_prob_distribution, logits=predict_logits)\n",
    "\n",
    "sess.run(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PTB数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将单词转化成模型可以读入的单词序列，将这10000个不同的词汇分别映射到0~9999之间的整数编号\n",
    "# 按照词频顺序为每个词分配一个编号，然后将词汇表保存到一个独立的covab文件\n",
    "\n",
    "import codecs\n",
    "import collections\n",
    "from operator import itemgetter\n",
    "\n",
    "RAW_DATA = \"./data/ptb.train.txt\"  # 训练集数据文件，低频词已经转换成\"<unk>\"\n",
    "VOCAB_OUTPUT = \"./data/ptb.vocab\"  # 输出的词汇表文件\n",
    "\n",
    "counter = collections.Counter()\n",
    "\n",
    "with codecs.open(RAW_DATA, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        for word in line.strip().split():\n",
    "            counter[word] += 1\n",
    "            \n",
    "# 按词频顺序对单词进行排序\n",
    "sorted_word_to_cnt = sorted(counter.items(), key=itemgetter(1), reverse=True)\n",
    "sorted_words = [x[0] for x in sorted_word_to_cnt]\n",
    "\n",
    "# 稍后需要在文本换行处加入句子结束符\"<eos>\"，预先将其加入词汇表\n",
    "sorted_words = [\"<eos>\"] + sorted_words \n",
    "\n",
    "with codecs.open(VOCAB_OUTPUT, \"w\", encoding=\"utf-8\") as file_output:\n",
    "    for word in sorted_words:\n",
    "        file_output.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确定词汇表之后，将训练文件，测试文件等根据词汇文件转化为单词编号，每个单词的编号就是它在词汇文件中的行号\n",
    "\n",
    "import codecs\n",
    "import sys\n",
    "\n",
    "RAW_DATA=\"./data/ptb.train.txt\"  # 原始训练集数据文件\n",
    "VOCAB = \"./data/ptb.vocab\"       #  词汇表文件\n",
    "OUTPUT_DATA = \"./data/ptb.train\"    # 将单词替换为单词编号后的输出文件\n",
    "\n",
    "# 读取词汇表，并建立词汇到单词编号的映射\n",
    "with codecs.open(VOCAB, \"r\", encoding=\"utf-8\") as f_vocab:\n",
    "    vocab = [w.strip() for w in f_vocab.readlines()]\n",
    "word_to_id = {k:v for (k,v) in zip(vocab, range(len(vocab)))}\n",
    "\n",
    "# 如果出现了被删除的低频词，则替换为\"<unk>\"\n",
    "def get_id(word):\n",
    "    return word_to_id[word] if word in word_to_id else word_to_id[\"<unk>\"]\n",
    "\n",
    "fin = codecs.open(RAW_DATA, \"r\", encoding=\"utf-8\")\n",
    "fout = codecs.open(OUTPUT_DATA, \"w\", encoding=\"utf-8\")\n",
    "for line in fin:\n",
    "    words = line.strip().split() + [\"<eos>\"]\n",
    "    # 将每个单词替换为词汇表中的编号\n",
    "    out_line = \" \".join([str(get_id(w)) for w in words]) + \"\\n\"\n",
    "    fout.write(out_line)\n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PTB数据batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "TRAIN_DATA = \"./data/ptb.train\"  #使用单词编号便是的训练数据\n",
    "TRAIN_BATCH_SIZE = 20\n",
    "TRAIN_NUM_STEP = 35\n",
    "\n",
    "# 从文件中读取数据，并返回包含单词编号的数据\n",
    "def read_data(file_path):\n",
    "    with open(file_path, \"r\") as fin:\n",
    "        # 将整个文档读进一个长字符串\n",
    "        id_string=\" \".join([line.strip() for line in fin.readlines()])\n",
    "    id_list = [int(w) for w in id_string.split()]  # 将读取的单词编号转为整数\n",
    "    return id_list\n",
    "\n",
    "def make_batches(id_list, batch_size, num_step):\n",
    "    # 计算总的batch数量。每个batch包含的单词数量是batch_size * num_step。\n",
    "    num_batches = (len(id_list)-1) // (batch_size*num_step)\n",
    "    \n",
    "    # 数据整理成维度为[batch_size, num_batches*num_step]的二维数组\n",
    "    data = np.array(id_list[:num_batches * batch_size * num_step])\n",
    "    data = np.reshape(data, [batch_size, num_batches * num_step])\n",
    "    # 沿着第二个维度将数据切分成num_batches个batch,存入一个数组\n",
    "    data_batches = np.split(data, num_batches, axis=1)\n",
    "\n",
    "    # 重复上述操作，但是每个位置向右移动一位。这里得到的是RNN每一步输出所需要预测的下一个单词。\n",
    "    label = np.array(id_list[1: num_batches * batch_size * num_step + 1])\n",
    "    label = np.reshape(label, [batch_size, num_batches * num_step])\n",
    "    label_batches = np.split(label, num_batches, axis=1)\n",
    "    # 返回一个长度为num_batches的数组，其中每一项包括一个data矩阵和一个label矩阵\n",
    "    return list(zip(data_batches, label_batches))\n",
    "\n",
    "def main():\n",
    "    train_batches = make_batches(read_data(TRAIN_DATA),\n",
    "                                TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### embedding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.get_variable(\"embedding\", [VOCAB_SIZE, EMB_SIZE])\n",
    "# 输出的矩阵比输入数据多一个维度，新增维度的大小是EMB_SIZE\n",
    "# 一般input_data的维度是batch_size * num_step\n",
    "# 而输出的input_embedding维度是batch_size * num_step * EMB_SIZE\n",
    "input_embedding = tf.nn.embedding_lookup(embedding, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### softmax层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义线性映射用到的参数\n",
    "# HIDDEN_SIZE是循环神经网络的隐藏状态维度，VOCAB_SIZE是词汇表的大小\n",
    "weight = tf.get_variable(\"weight\", [HIDDEN_SIZE, COVAB_SIZE])\n",
    "bias = tf.get_variable(\"bias\", [VOCAB_SIZE])\n",
    "# 计算线性映射\n",
    "# output是RNN的输出，维度为[batch_size * num_steps, HIDDEN_SIZE]\n",
    "logits = tf.nn.bias_add(tf.matmul(output, weight), bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs的维度与logits的维度相同\n",
    "probs = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label是一个大小为[batch_size * num_steps]的一组数组，包含每个位置正确的单词编号\n",
    "# logits维度是[batch_size * num_steps, HIDDEN_SIZE]\n",
    "# loss维度与labels相同，代表每个位置上的log perplexity\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(self.targets, [-1]), logits=logitts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

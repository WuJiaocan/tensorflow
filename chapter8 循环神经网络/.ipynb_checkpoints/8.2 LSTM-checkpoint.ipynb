{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义LSTM网络\n",
    "\n",
    "# LSTM中使用的变量也会在该函数中自动被声明\n",
    "lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_hidden_size)\n",
    "\n",
    "# 将LSTM中的状态初始化为全0数组。BasicLSTMCell类提供了zero_state函数来生成全零的初始状态。state是一个包含两个张量的LSTMStateTuple类，\n",
    "# 其中state.c和state.h分别对应了c状态和h状态\n",
    "# 优化循环神经网络时，每次也会使用一个batch的训练样本\n",
    "state = lstm.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# 定义损失函数\n",
    "loss = 0.0\n",
    "\n",
    "# num_steps为训练数据的长度，为了将循环神经网络展开成前馈神经网络。\n",
    "for i in range(num_steps):\n",
    "    \n",
    "    # 在第一个时刻声明LSTM结构中使用的变量，在之后的时刻都需要复用之前定义好的变量\n",
    "    if i > 0: tf.get_variable_scope().reuse_varibales()\n",
    "        \n",
    "    # 每一个处理时间序列中的一个时刻。\n",
    "    # 将当前输入current_input和前一时刻状态state(ht-1和Ct-1)传入定义的LSTM结构，得到当前LSTM的输出lstm_output(ht)和更新后的状态state(ht和hc)\n",
    "    # lstm_output用于输出给其他层，state用于输出给下一时刻，它们在dropout等方面可以有不同的处理方式\n",
    "    lstm_output, state = lstm(current_input, state)\n",
    "    \n",
    "    # 将当前时刻LSTM结构的输出传入一个全连接层得到最后的输出\n",
    "    final_output = fully_connected(lstm_output)\n",
    "    \n",
    "    # 计算当前时刻输出的损失\n",
    "    loss += calc_loss(final_output, expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
